{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv. neuronal network sentence classifier notebook\n",
    "In this notebook we will attemp to build a sentence classifier model based on https://arxiv.org/pdf/1408.5882.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Packages setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by installing a set of requerired packages, we need to override some exiting package installations (e.g. fairing 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import site\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = str(Path.home())\n",
    "local_py_path = os.path.join(home, \".local/lib/python3.6/site-packages\")\n",
    "if local_py_path not in sys.path:\n",
    "    logging.info(\"Adding %s to python path\", local_py_path)\n",
    "    sys.path.insert(0, local_py_path)\n",
    "site.getsitepackages()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"):\n",
    "    raise ValueError(\"Notebook is missing google application credentials\")\n",
    "else:\n",
    "    print('GCP Credentials OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade \n",
    "!pip install --user pandas\n",
    "!pip install --user tensorflow\n",
    "!pip install --user keras\n",
    "!pip install --user numpy\n",
    "!pip install --user gcsfs\n",
    "!pip install --user google-cloud-storage\n",
    "!pip install --user gensim\n",
    "!pip install --user kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install a recent fairing commit, we hit a couple of bugs with the released one: https://github.com/kubeflow/kubeflow/issues/3643 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user git+git://github.com/kubeflow/fairing.git@dc61c4c88f233edaf22b13bbfb184ded0ed877a4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading train and test data files from Google Cloud Storage. We will using the Wikipedia Movies Plot Dataset (https://www.kaggle.com/jrobischon/wikipedia-movie-plots). This dataset features plot summary descriptions scraped from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'data/wiki_movie_plots_deduped.csv'\n",
    "test_data_path = 'data/wiki_movie_plots_deduped_test.csv'\n",
    "gcp_bucket = 'velascoluis-test'\n",
    "column_target_value = 'Genre'\n",
    "column_text_value = 'Plot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop rows with missing data and deduplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_load = pd.read_csv(\"gs://\" + gcp_bucket + \"/\" + train_data_path, sep=',')\n",
    "test_data_load = pd.read_csv(\"gs://\" + gcp_bucket + \"/\" + test_data_path, sep=',')\n",
    "train_data = train_data_load.dropna().drop_duplicates(subset=column_text_value, keep='first', inplace=False)\n",
    "test_data = test_data_load.dropna().drop_duplicates(subset=column_text_value, keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a glimpse of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on two columns: Plot and Genre. The algorithm goal will be to infer the movie genre based on the plot.\n",
    "Next step is to drop rows with unknown genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data.Genre != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exploring the histogram of genres distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data[column_target_value], color = 'blue', edgecolor = 'black')\n",
    "plt.title('Histogram of movies by genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is severely swekedm and we have a long tail of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[column_target_value].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus only on the genres featuring at least 900 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.groupby(column_target_value).filter(lambda x : len(x)>900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[column_target_value].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to balance the data, we will randomly trim data from the drama and comeny genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(((train_data[train_data[column_target_value] == 'drama' ]).sample(frac=.8,random_state=200))).index)\n",
    "train_data = train_data.drop(((train_data[train_data[column_target_value] == 'comedy' ]).sample(frac=.75,random_state=200))).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[column_target_value].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data[column_target_value], color = 'blue', edgecolor = 'black')\n",
    "plt.title('Histogram of movies by genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_values = train_data[column_target_value].unique()\n",
    "print(classifier_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will generate numerical labels for the genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for i, class_value in enumerate(classifier_values):\n",
    "    dic[class_value] = i\n",
    "labels = train_data[column_target_value].apply(lambda x: dic[x])\n",
    "num_classes = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also split the data between training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_pct = 0.2\n",
    "val_data = train_data.sample(frac=val_data_pct, random_state=200)\n",
    "train_data = train_data.drop(val_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, will be generating representations of the sentences to classify, we create a vocabulary index based on word frequency and then transform the text to numerical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "texts = train_data[column_text_value]\n",
    "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "sequences_valid = tokenizer.texts_to_sequences(val_data[column_text_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we pad the sequences so all of them will have the same lenght, for the labels text we create categorical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences_train)\n",
    "sequence_length = x_train.shape[1]\n",
    "x_val = pad_sequences(sequences_valid, maxlen=sequence_length)\n",
    "y_train = to_categorical(np.asarray(labels[train_data.index]))\n",
    "y_val = to_categorical(np.asarray(labels[val_data.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will generate the word embeddings, we will use transfer learning and re-use a pretrained word2vec model. In this case we will use GloVe (https://nlp.stanford.edu/projects/glove/) 100 dimensions. We had to tranform the Glove representation to word2vec using the glove2word2vec util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "w2v_model_path = 'model/word2vec100d.txt'\n",
    "w2v_model = word2vec.KeyedVectors.load_word2vec_format(\"gs://\" + gcp_bucket + \"/\" + w2v_model_path)\n",
    "word_vectors = w2v_model.wv\n",
    "word_index = tokenizer.word_index\n",
    "vocabulary_size = min(len(tokenizer.word_index) + 1, num_words)\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar('summer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.Model generation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate, Reshape, \\\n",
    "    Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the model using the Keras functional API, in essence the model is made of an Embedding  + Conv  + Maxpool + Dense layers. The tunnable hps are:\n",
    "- number of convolutions + maxpool\n",
    "- filter sizes, we add 3,5,7 ..\n",
    "- number of filters\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 3\n",
    "activation_conv = 'relu'\n",
    "activation_max = 'softmax'\n",
    "drop = 0.5\n",
    "num_filters = 200\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size, embedding_dim, weights=[embedding_matrix], trainable=True)\n",
    "inputs = Input(shape=(sequence_length))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding)\n",
    "conv_layer_1 = Conv2D(num_filters, (3, embedding_dim),\n",
    "                    activation=activation_conv,kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "maxpool_layer_1 = MaxPooling2D((sequence_length - 3 + 1, 1), \n",
    "                             strides=(1,1))(conv_layer_1)\n",
    "\n",
    "flatten = Flatten()(maxpool_layer_1)\n",
    "reshape = Reshape((num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=num_classes, activation=activation_max, kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "model = Model(inputs, output)\n",
    "adam = Adam(lr=1e-3)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 500\n",
    "now = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"model/tf_logs\"\n",
    "if not os.path.exists(root_logdir):\n",
    "    os.mkdir(root_logdir)\n",
    "log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "callback_tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "callback_earlystopping = EarlyStopping(monitor='val_loss')\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val),\n",
    "                  callbacks=[callback_earlystopping, callback_tensorboard])\n",
    "loss, acc = model.evaluate(x_train, y_train, verbose=2)\n",
    "print(\"Accuracy = {:5.2f}%\".format(100 * acc))\n",
    "print(\"Loss = {:5.2f}%\".format(100 * loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the accuracy we will use katib to search the optima hyperparmeters for us. First we need to build an image with the training code. Note we install a specific version of fairing, otherwise we may hit https://github.com/kubeflow/kubeflow/issues/3643\n",
    "This is adapted from https://github.com/jlewi/examples/blob/hptuning/xgboost_synthetic/build-train-deploy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.fairing import cloud\n",
    "from kubeflow.fairing.builders import append\n",
    "from kubeflow.fairing.builders import cluster\n",
    "from kubeflow.fairing.deployers import job\n",
    "from kubeflow.fairing import utils\n",
    "from kubeflow.fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_project = cloud.gcp.guess_project_name()\n",
    "docker_registry = 'gcr.io/{}/text-cnn-class-dev'.format(gcp_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a wapper class for launching the k8s job, and the re-arrange the code around functions. The main change is adding the configurable hyper-parameters in the generate_model func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import json\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate, Reshape, \\\n",
    "    Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers        \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transform the next class to a CLI callable python file, basically we will be able to do python3 <nb_name.py> launch_rig --param1=value1 ... paramN=valueN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "class CNN():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sequence_length = None\n",
    "        self.num_classes = None\n",
    "        self.word_index = None\n",
    "        self.class_values_list = None\n",
    "        self.vocabulary_size = None\n",
    "\n",
    "    def prepare_data_train(self, num_words, train_data_path, test_data_path, column_target_value, column_text_value,\n",
    "                           val_data_pct, json_tokenizer_path,gcp_bucket):\n",
    "        \n",
    "        train_data_load = pd.read_csv(\"gs://\" + gcp_bucket + \"/\" + train_data_path, sep=',')\n",
    "        test_data_load = pd.read_csv(\"gs://\" + gcp_bucket + \"/\" + test_data_path, sep=',')\n",
    "        train_data = train_data_load.dropna().drop_duplicates(subset=column_text_value, keep='first', inplace=False)\n",
    "        test_data = test_data_load.dropna().drop_duplicates(subset=column_text_value, keep='first', inplace=False)\n",
    "        train_data = train_data[train_data.Genre != 'unknown']\n",
    "        train_data = train_data.groupby(column_target_value).filter(lambda x : len(x)>900)\n",
    "        train_data = train_data.drop(((train_data[train_data[column_target_value] == 'drama' ]).sample(frac=.8)).index)\n",
    "        train_data = train_data.drop(((train_data[train_data[column_target_value] == 'comedy' ]).sample(frac=.75)).index)\n",
    "        classifier_values = train_data[column_target_value].unique()\n",
    "        dic = {}\n",
    "        for i, class_value in enumerate(classifier_values):\n",
    "            dic[class_value] = i\n",
    "        labels = train_data[column_target_value].apply(lambda x: dic[x])\n",
    "        val_data = train_data.sample(frac=val_data_pct, random_state=200)\n",
    "        train_data = train_data.drop(val_data.index)\n",
    "        texts = train_data[column_text_value]\n",
    "        tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        tokenizer_json = tokenizer.to_json()\n",
    "        if not os.path.exists(os.path.dirname(json_tokenizer_path)):\n",
    "            os.mkdir(os.path.dirname(json_tokenizer_path))\n",
    "        with io.open(json_tokenizer_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "        sequences_valid = tokenizer.texts_to_sequences(val_data[column_text_value])\n",
    "        x_train = pad_sequences(sequences_train)\n",
    "        x_val = pad_sequences(sequences_valid, maxlen=x_train.shape[1])\n",
    "        y_train = to_categorical(np.asarray(labels[train_data.index]))\n",
    "        y_val = to_categorical(np.asarray(labels[val_data.index]))\n",
    "        self.sequence_length = x_train.shape[1]\n",
    "        self.num_classes = i + 1\n",
    "        self.word_index = tokenizer.word_index\n",
    "        self.class_values_list = classifier_values\n",
    "        return (x_train, x_val, y_train, y_val)\n",
    "\n",
    "    def prepare_embeddings(self, num_words, w2v_model_path, embedding_dim,gcp_bucket):\n",
    "        \n",
    "        \n",
    "        w2v_model = word2vec.KeyedVectors.load_word2vec_format(\"gs://\" + gcp_bucket + \"/\" + w2v_model_path)\n",
    "        word_vectors = w2v_model.wv\n",
    "        vocabulary_size = min(len(self.word_index) + 1, num_words)\n",
    "        embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "        for word, i in self.word_index.items():\n",
    "            if i >= num_words:\n",
    "                continue\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n",
    "        del (word_vectors)\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        return (embedding_matrix)\n",
    "\n",
    "    def generate_keras_model(self, num_conv_layers, maxpool_strides, drop,num_filters, embedding_dim,\n",
    "                             embedding_matrix):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        embedding_layer = Embedding(self.vocabulary_size, embedding_dim, weights=[embedding_matrix], trainable=True)\n",
    "        inputs = Input(shape=(self.sequence_length,))\n",
    "        embedding = embedding_layer(inputs)\n",
    "        reshape = Reshape((self.sequence_length, embedding_dim, 1))(embedding)\n",
    "        filter_sizes = []\n",
    "        for i in range(0, num_conv_layers*2 - 1, 2):\n",
    "            filter_sizes.append(i+3)\n",
    "        convolutions = []\n",
    "        for layer_index in range(num_conv_layers):\n",
    "            conv_layer = Conv2D(num_filters, (filter_sizes[layer_index], embedding_dim), activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "            maxpool_layer = MaxPooling2D((self.sequence_length - filter_sizes[layer_index] + 1, 1), strides=(maxpool_strides[0], maxpool_strides[1]))(\n",
    "                conv_layer)\n",
    "            convolutions.append(maxpool_layer)\n",
    "        if (num_conv_layers > 1):\n",
    "            merged_tensor = concatenate(convolutions, axis=1)\n",
    "        else:\n",
    "            merged_tensor = convolutions[0]\n",
    "        flatten = Flatten()(merged_tensor)\n",
    "        reshape = Reshape((num_conv_layers * num_filters,))(flatten)\n",
    "        dropout = Dropout(drop)(flatten)\n",
    "        output = Dense(units=self.num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "        model = Model(inputs, output)\n",
    "        adam = Adam(lr=1e-3)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=adam,\n",
    "                      metrics=['acc'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, x_train, x_val, y_train, y_val, batch_size, epochs):\n",
    "        \n",
    "        now = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"model/tf_logs\"\n",
    "        if not os.path.exists(root_logdir):\n",
    "            os.mkdir(root_logdir)\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "        callback_tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        callback_earlystopping = EarlyStopping(monitor='val_loss')\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val),\n",
    "                  callbacks=[callback_earlystopping, callback_tensorboard])\n",
    "        loss, acc = model.evaluate(x_train, y_train, verbose=2)\n",
    "        print(\"accuracy = {:5.2f}%\".format(100 * acc))\n",
    "                   \n",
    "    def launch_rig(self,train_data_path,test_data_path,column_target_value,column_text_value,\n",
    "                   json_tokenizer_path,gcp_bucket,w2v_model_path,\n",
    "                   num_conv_layers,dropout,num_filters,batch_size,epochs):\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logging.info('Arguments:')\n",
    "        \n",
    "        logging.info('train_data_path:{}'.format(train_data_path))\n",
    "        logging.info('test_data_path:{}'.format(test_data_path))\n",
    "        logging.info('column_target_value:{}'.format(column_target_value))\n",
    "        logging.info('column_text_value:{}'.format(column_text_value))\n",
    "        logging.info('json_tokenizer_path:{}'.format(json_tokenizer_path))\n",
    "        logging.info('gcp_bucket:{}'.format(gcp_bucket))\n",
    "        logging.info('w2v_model_path:{}'.format(w2v_model_path))\n",
    "        logging.info('num_conv_layers:{}'.format(num_conv_layers))\n",
    "        logging.info('dropout:{}'.format(dropout))\n",
    "        logging.info('num_filters:{}'.format(num_filters))\n",
    "        logging.info('batch_size:{}'.format(batch_size))\n",
    "        logging.info('epochs:{}'.format(epochs))\n",
    "        \n",
    "        x_train, x_val, y_train, y_val = self.prepare_data_train(10000,train_data_path,test_data_path,column_target_value,column_text_value,0.2,json_tokenizer_path,gcp_bucket)\n",
    "        embedding_matrix = self.prepare_embeddings(10000,w2v_model_path,100,gcp_bucket)\n",
    "        model = self.generate_keras_model(num_conv_layers,[1,1],dropout,num_filters,100,embedding_matrix)\n",
    "        self.train_model(model,x_train,x_val,y_train,y_val,batch_size,epochs)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local test to make sure the launch_rig function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "num_words = 10000\n",
    "train_data_path = 'data/wiki_movie_plots_deduped.csv'\n",
    "test_data_path = 'data/wiki_movie_plots_deduped_test.csv'\n",
    "gcp_bucket = 'velascoluis-test'\n",
    "column_target_value = 'Genre'\n",
    "column_text_value = 'Plot'\n",
    "val_data_pct = 0.2\n",
    "json_tokenizer_path = 'model/tokens.json'\n",
    "w2v_model_path = 'model/word2vec100d.txt'\n",
    "embedding_dim = 100\n",
    "num_conv_layers = 3\n",
    "maxpool_strides = [1,1]\n",
    "dropout = 0.5\n",
    "num_filters = 200\n",
    "batch_size = 300\n",
    "epochs = 15\n",
    "#Sequence\n",
    "CNN_instance = CNN()\n",
    "CNN_instance.launch_rig(train_data_path,test_data_path,column_target_value,column_text_value,json_tokenizer_path,gcp_bucket,w2v_model_path,num_conv_layers,dropout,num_filters,batch_size,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the preprocessor marking requirements.txt as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire(\"CNN\")\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files=[\"requirements.txt\"]\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small setup for getting to the docker repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker --quiet\n",
    "!gcloud auth activate-service-account --key-file=/secret/gcp/user-gcp-sa.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the base image and push it to GCR, note we are using a custom dockerfile for the image build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = \"tensorflow/tensorflow:latest-py3\"\n",
    "namespace= \"kubeflow-velascoluis\"\n",
    "cluster_builder = cluster.cluster.ClusterBuilder(registry=docker_registry,\n",
    "                                                 base_image=base_image,\n",
    "                                                 dockerfile_path='Dockerfile',\n",
    "                                                 preprocessor=preprocessor,\n",
    "                                                 pod_spec_mutators=[cloud.gcp.add_gcp_credentials_if_exists],\n",
    "                                                 namespace=namespace,\n",
    "                                                 context_source=cluster.gcs_context.GCSContextSource())\n",
    "cluster_builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute only the AppendBuilder if made changes of the CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.preprocess()\n",
    "builder = append.append.AppendBuilder(registry=docker_registry,\n",
    "                                      base_image=cluster_builder.image_tag, preprocessor=preprocessor)\n",
    "builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.image_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.executable.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_image(raw_yaml, image):\n",
    "    \"\"\"Set the container image given raw yaml.\n",
    "    \n",
    "    Args:\n",
    "      raw_yaml: A string containing raw YAML for a job\n",
    "      image: The docker image to use\n",
    "    \"\"\"\n",
    "    lines = raw_yaml.splitlines()\n",
    "    \n",
    "    for i, l in enumerate(lines):\n",
    "        if l.strip().startswith(\"image:\"):\n",
    "            lines[i] = l.split(\":\", 1)[0] + \":\" + \" \" + image\n",
    "            \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the katib job, we use the random algorithm, it will basically select a number of random states in the search space. For mor algorithms (TPE, hyperband, grid search ..) see https://github.com/kubeflow/katib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "hp_experiment_raw = \"\"\"\n",
    "apiVersion: \"kubeflow.org/v1alpha3\"\n",
    "kind: Experiment\n",
    "metadata:\n",
    "  labels:\n",
    "    controller-tools.k8s.io: \"1.0\"\n",
    "spec:\n",
    "  objective:\n",
    "    type: maximize\n",
    "    goal: 0.99\n",
    "    objectiveMetricName: accuracy\n",
    "    additionalMetricNames:\n",
    "      - train-accuracy\n",
    "  algorithm:\n",
    "    algorithmName: random\n",
    "  trialTemplate:\n",
    "    goTemplate:\n",
    "      rawTemplate:\n",
    "  parallelTrialCount: 2\n",
    "  maxTrialCount: 4\n",
    "  metricsCollectorSpec:\n",
    "    collector:\n",
    "      kind: StdOut\n",
    "    objective:\n",
    "      additionalMetricNames:\n",
    "        - accuracy\n",
    "  maxFailedTrialCount: 1\n",
    "  parameters:\n",
    "    - name: \"--num_filters\"\n",
    "      parameterType: int\n",
    "      feasibleSpace:\n",
    "        min: \"80\"\n",
    "        max: \"150\"\n",
    "    - name: \"--num_conv_layers\"\n",
    "      parameterType: int\n",
    "      feasibleSpace:\n",
    "        min: \"2\"\n",
    "        max: \"3\"\n",
    "    - name: \"--dropout\"\n",
    "      parameterType: double\n",
    "      feasibleSpace:\n",
    "        min: \"0.45\"\n",
    "        max: \"0.55\"      \n",
    "\"\"\"        \n",
    "\n",
    "# The batch job that will be launched on each trial\n",
    "# \n",
    "trial_job_raw = \"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {{.Trial}}\n",
    "  namespace: {{.NameSpace}}\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: {{.Trial}}\n",
    "        image: xxx\n",
    "        workingDir: /app\n",
    "        command:\n",
    "        - \"python\"\n",
    "        - \"TextClassifier.py\"\n",
    "        - launch_rig\n",
    "        - \"--train_data_path=data/wiki_movie_plots_deduped.csv\"\n",
    "        - \"--test_data_path=data/wiki_movie_plots_deduped_test.csv\"\n",
    "        - \"--column_target_value=Genre\"\n",
    "        - \"--column_text_value=Plot\"\n",
    "        - \"--json_tokenizer_path=model/tokens.json\"\n",
    "        - \"--gcp_bucket=velascoluis-test\"\n",
    "        - \"--w2v_model_path=model/word2vec100d.txt\"\n",
    "        - \"--batch_size=300\"\n",
    "        - \"--epochs=15\"        \n",
    "        {{- with .HyperParameters}}\n",
    "        {{- range .}}\n",
    "        - \"{{.Name}}={{.Value}}\"\n",
    "        {{- end}}\n",
    "        {{- end}}\n",
    "      restartPolicy: Never\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "hp_experiment = yaml.load(hp_experiment_raw)\n",
    "hp_experiment[\"metadata\"][\"namespace\"] = utils.get_current_k8s_namespace()\n",
    "trial_job_raw = set_image(trial_job_raw, builder.image_tag)\n",
    "hp_experiment[\"spec\"][\"trialTemplate\"][\"goTemplate\"][\"rawTemplate\"] = trial_job_raw\n",
    "\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "hp_experiment[\"metadata\"][\"name\"] = \"cnn-text-exp-gpu-{0}\".format(now)\n",
    "print(yaml.safe_dump(hp_experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client as k8s_client\n",
    "client = k8s_client.ApiClient()\n",
    "crd_api = k8s_client.CustomObjectsApi(client)\n",
    "\n",
    "group, version = hp_experiment['apiVersion'].split('/')\n",
    "\n",
    "result = crd_api.create_namespaced_custom_object(\n",
    "  group=group,\n",
    "  version=version,\n",
    "  namespace=hp_experiment[\"metadata\"][\"namespace\"],\n",
    "  plural='experiments',\n",
    "  body=hp_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View status, the completion should take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crd_api.get_namespaced_custom_object(\n",
    "  group=group,\n",
    "  version=version,\n",
    "  namespace=hp_experiment[\"metadata\"][\"namespace\"],\n",
    "  plural='experiments',\n",
    "  name=hp_experiment[\"metadata\"][\"name\"])\n",
    "\n",
    "print(yaml.dump(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
