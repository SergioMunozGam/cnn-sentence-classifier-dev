{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction rig experiments and random thoughts\n",
    "\n",
    "(WIP)\n",
    "The prediction rig is a component a bit overlooked when designing production scale ML platforms.\n",
    "Other parts like components for supporting EdA or training tend to get more engineering focus.\n",
    "But at the end of the days, the value generation process occur precisely at predicion phase and is also the external interface for investments on ML.\n",
    "I think that inside the rig there should be at least the following components:\n",
    "\n",
    "- A prediction endpoint, of course, with capabilities for:\n",
    "    - Running low latency predictions\n",
    "    - Scalability, fault tolerance ..\n",
    "    - Advance loggig capabilities particulary important for ground truth checking\n",
    "    - A/B canary rollout capabilities\n",
    "    - Explanability\n",
    "- A feature transformer:\n",
    "    - In online models there tend to be a gap between the raw data and the data used for running the prediction.\n",
    "- A prediction transformer:\n",
    "    - blah\n",
    "- A model warmer:\n",
    "    - blah\n",
    "- A Feature Store for decoupling data producers and features\n",
    "    - Online\n",
    "    - Batch\n",
    "    - blah blah \n",
    "- A model (de)promoter\n",
    "\n",
    "..insert diagram of a full prediction rig\n",
    "\n",
    "This NB currenty shows:\n",
    "\n",
    "- [X] Use of KFServing \n",
    "- [ ] Use of KFServing with data transformer\n",
    "- [ ] Use of KFServing together with TF model warmer\n",
    "- [X] Use of FEAST for online pred\n",
    "- [ ] Use of FEAST for batch pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by generating a simple regressor model, using a Keras dataset.\n",
    "Regressor miles per gallon (MPG) based on car type.\n",
    "Taken from https://www.tensorflow.org/tutorials/keras/regression\n",
    "Deploy FEAST on GKE as explained on https://docs.feast.dev/installation/gke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -q seaborn\n",
    "!pip install --user feast\n",
    "!pip install --user kfserving\n",
    "!pip install --user google-cloud-storage\n",
    "!pip install --user wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import wget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.backend import get_session\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = wget.download(\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\", \"auto-mpg.data\")\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "sns.pairplot(dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels = test_dataset.pop('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats\n",
    "def norm(x):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n",
    "    Dense(64, activation=tf.nn.relu),\n",
    "    Dense(1)\n",
    "  ])\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "  normed_train_data, train_labels,\n",
    "  epochs=EPOCHS, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "print(normed_test_data)\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model\"\n",
    "version = \"1\"\n",
    "export_path = model_path+\"/\"+version\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "x = model.input\n",
    "y = model.output\n",
    "\n",
    "tensor_info_x = tf.saved_model.utils.build_tensor_info(x)\n",
    "tensor_info_y = tf.saved_model.utils.build_tensor_info(y)\n",
    "\n",
    "prediction_signature = (\n",
    "          tf.saved_model.signature_def_utils.build_signature_def(\n",
    "              inputs={'car_values': tensor_info_x},\n",
    "              outputs={'mpg': tensor_info_y},\n",
    "          method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "\n",
    "\n",
    "builder.add_meta_graph_and_variables(\n",
    "      sess, [tf.saved_model.tag_constants.SERVING],\n",
    "      signature_def_map={\n",
    "          'predict_car_values':\n",
    "              prediction_signature\n",
    "      },\n",
    "      main_op=tf.global_variables_initializer(),\n",
    "      strip_default_attrs=True)\n",
    "\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not use this, we need to get the backend tf sess and generte the signatureDef, otherwise it fails! This is the only way I found to get Keras support\n",
    "#model.save(\"model/1\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir \"model/1\" --tag_set serve --signature_def predict_car_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(\"velascoluis-test\")\n",
    "if bucket:\n",
    "    for root, _, files in os.walk(\"model/1\"):\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            blob = bucket.blob(\"mpg/model/1/\"+file)\n",
    "            print('Uploading ..' + path)\n",
    "            blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kfserving import KFServingClient\n",
    "from kfserving import constants\n",
    "from kfserving import V1alpha2EndpointSpec\n",
    "from kfserving import V1alpha2PredictorSpec\n",
    "from kfserving import V1alpha2TensorflowSpec\n",
    "from kfserving import V1alpha2InferenceServiceSpec\n",
    "from kfserving import V1alpha2InferenceService\n",
    "from kubernetes.client import V1ResourceRequirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = constants.KFSERVING_GROUP + '/' + constants.KFSERVING_VERSION\n",
    "now = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "inference_service_name = 'pred111'\n",
    "default_endpoint_spec = V1alpha2EndpointSpec(\n",
    "    predictor=V1alpha2PredictorSpec(\n",
    "    tensorflow=V1alpha2TensorflowSpec(\n",
    "    storage_uri=\"gs://velascoluis-test/mpg/model\",\n",
    "    resources=V1ResourceRequirements(\n",
    "    requests={'cpu': '100m', 'memory': '1Gi'},\n",
    "    limits={'cpu': '100m', 'memory': '1Gi'}))))\n",
    "\n",
    "isvc = V1alpha2InferenceService(api_version=api_version,\n",
    "                                    kind=constants.KFSERVING_KIND,\n",
    "                                    metadata=\n",
    "                                        client.V1ObjectMeta(\n",
    "                                            name=inference_service_name,\n",
    "                                            annotations=\n",
    "                                            {\n",
    "                                                'sidecar.istio.io/inject': 'false',\n",
    "                                                'autoscaling.knative.dev/target': '1'\n",
    "                                            },\n",
    "                                            namespace=\"kubeflow-velascoluis\"\n",
    "                                                            ),\n",
    "                                    spec=\n",
    "                                        V1alpha2InferenceServiceSpec(default=default_endpoint_spec))\n",
    "\n",
    "# Idea is to insert here the transformer or should it be at kafka level in feast????\n",
    "#velascoluis: sidecar is disables by https://github.com/knative/serving/issues/6829\n",
    "\n",
    "\n",
    "KFServing = KFServingClient()\n",
    "#KFServing.set_credentials(storage_type='GCS',\n",
    "#                          namespace='kubeflow-velascoluis',\n",
    "#                          credentials_file=os.environ['GOOGLE_APPLICATION_CREDENTIALS'],\n",
    "#                          service_account='default-editor')\n",
    "\n",
    "KFServing.create(isvc)\n",
    "KFServing.get(inference_service_name, namespace=\"kubeflow-velascoluis\", watch=True, timeout_seconds=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v -H \"Host: pred111.kubeflow-velascoluis.example.com\" http://34.76.151.35/v1/models/pred111:predict -d \"{ \\\"signature_name\\\": \\\"predict_car_values\\\",  \\\"instances\\\":[[1.483887,      1.865988,    2.234620,  1.018782,     -2.530891,   -1.604642, -0.465148, -0.495225,  0.774676]]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytz import timezone, utc\n",
    "from feast import Client, FeatureSet, Entity, ValueType, Feature\n",
    "from feast.serving.ServingService_pb2 import GetOnlineFeaturesRequest\n",
    "from feast.types.Value_pb2 import Value as Value\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "from datetime import datetime, timedelta\n",
    "from random import randrange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAST_IP=\"35.241.140.170\"\n",
    "FEAST_CORE_URL=FEAST_IP+\":32090\"\n",
    "FEAST_ONLINE_SERVING_URL=FEAST_IP+\":32091\"\n",
    "FEAST_BATCH_SERVING_URL=FEAST_IP+\":32092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(core_url=FEAST_CORE_URL, serving_url=FEAST_ONLINE_SERVING_URL)\n",
    "client.create_project('feast_kfserving')\n",
    "client.set_project('feast_kfserving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ingest = train_dataset\n",
    "features_ingest['datetime']=np.random.choice(pd.date_range('2020-01-01', '2020-04-15'), len(features_ingest))\n",
    "features_ingest['car_id'] = np.arange(len(train_dataset_datetime))\n",
    "features_ingest.columns = map(str.lower, train_dataset_datetime.columns)\n",
    "features_ingest.columns = features_ingest.columns.str.replace(' ', '')\n",
    "print(features_ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_f = FeatureSet(\n",
    "    \"car_features\",\n",
    "    entities=[Entity(name='car_id', dtype=ValueType.INT64)],\n",
    "    max_age=Duration(seconds=432000)    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_f.infer_fields_from_df(features_ingest, replace_existing_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.apply(cars_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_features = client.get_feature_set(\"car_features\",version=1)\n",
    "print(car_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.ingest(\"car_features\", features_ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_features = client.get_online_features(\n",
    "    feature_refs=[\n",
    "        f\"cylinders\",\n",
    "        f\"displacement\",\n",
    "        f\"horsepower\",\n",
    "        f\"acceleration\",\n",
    "        f\"europe\",\n",
    "        f\"japan\",\n",
    "        f\"usa\",\n",
    "        \n",
    "    ],\n",
    "    entity_rows=[\n",
    "        GetOnlineFeaturesRequest.EntityRow(\n",
    "            fields={\n",
    "                \"car_id\": Value(\n",
    "                    int64_val=10)\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to call random here .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v -H \"Host: pred111.kubeflow-velascoluis.example.com\" http://34.76.151.35/v1/models/pred111:predict -d \"{ \\\"signature_name\\\": \\\"predict_car_values\\\",  \\\"instances\\\":[[\"+online_features+\"\"]]}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
